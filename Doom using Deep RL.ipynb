{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45713986",
   "metadata": {},
   "source": [
    " # Doom using RL\n",
    " \n",
    " ## 1. Install VizDoom\n",
    " \n",
    " \n",
    " Follow the instructions from the [git repo](https://github.com/Farama-Foundation/ViZDoom) to install the library, you will also need to clone it as the config (.cfg) and scenario (.wad) files are needed to define our environment. If this is being installed on your PC/Mac instead of a venv, please change to the VizDoom directory and navigate to the scenarios folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    " # (cd /Users/y-unus/ViZDoom/build/lib/vizdoom/scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910074cd",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352f74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box # for sampling actions and images (frames)\n",
    "import cv2 # preprocessing (greyscale)\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common import evaluation\n",
    "from stable_baselines3.common import policies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6545f",
   "metadata": {},
   "source": [
    "# 3. Initial test\n",
    "\n",
    "With the basic configuration, the agent and the enemy are spawned across from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8270e73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the game instance\n",
    "\n",
    "game = DoomGame()\n",
    "game.load_config('basic.cfg') # if you have not changed directories, mention the full path here\n",
    "game.init()\n",
    "\n",
    "# Define the actions (buttons available) the agent can perform\n",
    "\n",
    "actions = [\n",
    "        [1, 0, 0],  # Move left\n",
    "        [0, 1, 0],  # Move right\n",
    "        [0, 0, 1],  # Attack  \n",
    "]\n",
    "\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a7bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "Result: -385.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "Result: -385.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: 84.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: 90.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: -199.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: -110.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: -106.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: 92.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -6.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: -1.0\n",
      "reward: 100.0\n",
      "reward: -1.0\n",
      "Result: 49.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "# this is a buffer before an action is performed (# of tics) to help the agent with learning \n",
    "frame_skip = 4 \n",
    "\n",
    "for episode in range(episodes):\n",
    "    game.new_episode() # start a fresh episode/environment reset\n",
    "    \n",
    "    while not game.is_episode_finished():\n",
    "    # checks if the episode has ended (timesteps reached or close by user) \n",
    "       \n",
    "        # get the state from the environment\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # this is the amount of ammo from the cfg file (50 by default)\n",
    "        info = state.game_variables \n",
    "        \n",
    "        # Take action - currently random (exploration). Rewards are not being stored \n",
    "        reward = game.make_action(random.choice(actions), frame_skip)\n",
    "        \n",
    "        print('reward:', reward) # print reward at each step\n",
    "\n",
    "    print('Cumulative reward:', game.get_total_reward()) # Reward at the end of the episode\n",
    "\n",
    "game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37f97f",
   "metadata": {},
   "source": [
    "## 4. Create Gym Environment\n",
    "\n",
    "The above test does not use the gym environment for our interface and learning. Here we define the functions to peform the initialization, preprocessing steps and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0c6ebc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class start_game(Env):\n",
    "    \n",
    "    # Initialization - creates a new environment, define observation and action space\n",
    "    def __init__(self, scenario):\n",
    "        \n",
    "        super().__init__()\n",
    "        current_path = '/Users/y-unus/Documents/SS2023/CMSE890-730 - AML/Project - Doom RL'\n",
    "        cfg_file = os.path.join(current_path,scenario  + '.cfg')\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(cfg_file) # if you have not changed directories, mention the full path here\n",
    "            \n",
    "        self.game.init() # start\n",
    "        \n",
    "            # get the frame resolution from the current game and push color channel to last position in the tuple\n",
    "        self.obs_shape = self.game.get_state().screen_buffer.shape  # to read dynamic frame resolution; hardcoded for higer levels\n",
    "        self.obs_shape =  self.obs_shape[1:] + (self.obs_shape[0],)\n",
    "        self.observation_space = Box(low = 0, high = 255, shape =  self.obs_shape, dtype = np.uint8)\n",
    "#         self.observation_space = Box(low = 0, high = 255, shape =  (100,160,1), dtype = np.uint8)\n",
    "        \n",
    "        self.action_space = Discrete(3) \n",
    "\n",
    "\n",
    "    # Define action for exploration/ exploitation and return the next state, reward, done \n",
    "    # (game finished) and info (game variables available)\n",
    "    def step(self, action):\n",
    "\n",
    "        frame_skip = 4\n",
    "        \n",
    "        actions = [\n",
    "        [1, 0, 0],  # Move left\n",
    "        [0, 1, 0],  # Move right\n",
    "        [0, 0, 1],  # Attack  \n",
    "        ]\n",
    "        \n",
    "        reward = self.game.make_action(actions[action], frame_skip)\n",
    "        \n",
    "        if self.game.get_state(): # game is running\n",
    "\n",
    "            state = self.game.get_state().screen_buffer  # acquire next frame\n",
    "            state = self.preprocessing(state) # grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = {'Ammo': ammo}\n",
    "            \n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape) # game has terminated\n",
    "            info = {\"Ammo\": 50} \n",
    "        \n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info\n",
    "    \n",
    "    # Reset parameters for new episode and return the state\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        state = self.preprocessing(state) \n",
    "        return state\n",
    "    \n",
    "    # Convert the incoming frames to grayscale\n",
    "    def preprocessing(self, current_state):\n",
    "        observation = np.moveaxis(current_state, 0, -1) # invert the tuple\n",
    "        greyscale_img = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)   # we remove the RGB color channel \n",
    "        greyscale_img = cv2.resize( greyscale_img, (240,320), interpolation=cv2.INTER_CUBIC) \n",
    "        state = np.reshape(greyscale_img, (240,320,1))\n",
    "        return state\n",
    "\n",
    "    # closes game window after episodes are terminated to avoid manually closing it\n",
    "    def end_game(self):\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a24cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl1_checkpoint = './training/Basic'\n",
    "lvl1_log = './logs/Basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf56816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class save_my_model(BaseCallback):\n",
    "    \n",
    "# Define no of timesteps after which (check_freq) model is saved\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(save_my_model, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'Training_Model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# eval_env = DoomGame().load_config('basic.cfg')\n",
    "\n",
    "# Define no of timesteps (eval_freq) after which model is saved to path in model_checkpoint\n",
    "\n",
    "# callback = callbacks.EvalCallback(eval_env,\n",
    "#                                     n_eval_episodes=10,\n",
    "#                                              eval_freq=10000,\n",
    "#                                              log_path=log,\n",
    "#                                              best_model_save_path=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5e3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "basic_callback = save_my_model(check_freq=10000, save_path=lvl1_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa693ca",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5fb61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = start_game('basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "21626791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "train_model = PPO('CnnPolicy', train_env, tensorboard_log=lvl1_log, verbose=1, learning_rate=0.0015, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7911e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/Basic/PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 46.7        |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009782709 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.851      |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 551         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00266     |\n",
      "|    value_loss           | 2.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45.5        |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010595167 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.826      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 354         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    value_loss           | 1.28e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 41.5         |\n",
      "|    ep_rew_mean          | -142         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 472          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135607645 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.792       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 263          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | 0.0039       |\n",
      "|    value_loss           | 1.19e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 38.9         |\n",
      "|    ep_rew_mean          | -125         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 617          |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071525457 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.816       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 398          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.0039       |\n",
      "|    value_loss           | 1.8e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.9        |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 764         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011106117 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 451         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 1.04e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.1        |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 908         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035103373 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.898      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 339         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0069      |\n",
      "|    value_loss           | 764         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.7        |\n",
      "|    ep_rew_mean          | -51.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 7           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1057        |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101025596 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 197         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.0234      |\n",
      "|    value_loss           | 625         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 18.3       |\n",
      "|    ep_rew_mean          | 11.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 1203       |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03336873 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.904     |\n",
      "|    explained_variance   | 0.604      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.74e+03   |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.0353     |\n",
      "|    value_loss           | 3.89e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 14.8       |\n",
      "|    ep_rew_mean          | 28.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 7          |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1352       |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04913691 |\n",
      "|    clip_fraction        | 0.519      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.872     |\n",
      "|    explained_variance   | -0.274     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 681        |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | 0.0145     |\n",
      "|    value_loss           | 1.59e+03   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14c8d3610>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.learn(total_timesteps=10000, callback=basic_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2e979549",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = start_game('basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f43ef95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model_checkpoint path and specify the file to be used\n",
    "test_model = PPO.load('/Users/y-unus/Documents/SS2023/CMSE890-730 - AML/Project - Doom RL/training/Basic/best_model_30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0f5030ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/y-unus/miniforge3/envs/tensorflow/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.5"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluation.evaluate_policy(model, eval_env, n_eval_episodes=50)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "503b72a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.55912976558599"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261c258",
   "metadata": {},
   "source": [
    "Our agent is now a very decent performer, notice how the mean reward is no longer negative i.e., the agent is completing the objective of shooting down the enemy in under 20 seconds on average.\n",
    "\n",
    "## 6. Repurpose current policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d982287",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_checkpoint = './training/Lvl2'\n",
    "lvl2_log = './logs/Lvl2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e437e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lvl2_callback = save_my_model(check_freq=10000, save_path=lvl2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5adc2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_game = start_game('defend_the_center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86721f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_game.end_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "504616ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "train_model = PPO('CnnPolicy', lvl2_game, tensorboard_log=lvl2_log, verbose=1, learning_rate=0.0015, n_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "012f401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/Lvl2/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.7     |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 512      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.4        |\n",
      "|    ep_rew_mean          | 0.0769      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014622646 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.016       |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00982    |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 81.4        |\n",
      "|    ep_rew_mean          | 0.556       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015441906 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0721      |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.6        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019549541 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90.1       |\n",
      "|    ep_rew_mean          | 1.14       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 145        |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04270177 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.972     |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0845    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.1        |\n",
      "|    ep_rew_mean          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050599113 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.0982     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 0.0893      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 91.4       |\n",
      "|    ep_rew_mean          | 1.18       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 207        |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08281734 |\n",
      "|    clip_fraction        | 0.476      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.935     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0805    |\n",
      "|    value_loss           | 0.0676     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 93.5       |\n",
      "|    ep_rew_mean          | 1.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 239        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06799315 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.937     |\n",
      "|    explained_variance   | 0.566      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.119     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0774    |\n",
      "|    value_loss           | 0.0924     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 94.8       |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 270        |\n",
      "|    total_timesteps      | 4608       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13488612 |\n",
      "|    clip_fraction        | 0.513      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.878     |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0881    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 95.2       |\n",
      "|    ep_rew_mean          | 1.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 301        |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08272427 |\n",
      "|    clip_fraction        | 0.48       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.869     |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0709    |\n",
      "|    value_loss           | 0.0807     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 95.4       |\n",
      "|    ep_rew_mean          | 1.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 332        |\n",
      "|    total_timesteps      | 5632       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14031145 |\n",
      "|    clip_fraction        | 0.485      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.796     |\n",
      "|    explained_variance   | 0.6        |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0747    |\n",
      "|    value_loss           | 0.0922     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 96.8       |\n",
      "|    ep_rew_mean          | 1.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12787884 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.788     |\n",
      "|    explained_variance   | 0.689      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0714    |\n",
      "|    value_loss           | 0.0961     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 97.5       |\n",
      "|    ep_rew_mean          | 1.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 395        |\n",
      "|    total_timesteps      | 6656       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12819183 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.748     |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 98.1       |\n",
      "|    ep_rew_mean          | 1.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 430        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11403763 |\n",
      "|    clip_fraction        | 0.471      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.794     |\n",
      "|    explained_variance   | 0.782      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0975    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 0.0733     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 98.4      |\n",
      "|    ep_rew_mean          | 1.67      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 16        |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 465       |\n",
      "|    total_timesteps      | 7680      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1382858 |\n",
      "|    clip_fraction        | 0.527     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.813    |\n",
      "|    explained_variance   | 0.698     |\n",
      "|    learning_rate        | 0.0015    |\n",
      "|    loss                 | -0.0857   |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | -0.0832   |\n",
      "|    value_loss           | 0.0904    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 98.6       |\n",
      "|    ep_rew_mean          | 1.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 499        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10365273 |\n",
      "|    clip_fraction        | 0.548      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.863     |\n",
      "|    explained_variance   | 0.62       |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0884    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0735    |\n",
      "|    value_loss           | 0.101      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 98.8       |\n",
      "|    ep_rew_mean          | 1.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 533        |\n",
      "|    total_timesteps      | 8704       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13453507 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.898     |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0785    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0893    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 98.6       |\n",
      "|    ep_rew_mean          | 1.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 568        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14345703 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.815     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0989    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0879    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 99.2       |\n",
      "|    ep_rew_mean          | 1.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 603        |\n",
      "|    total_timesteps      | 9728       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13010171 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.814     |\n",
      "|    explained_variance   | 0.616      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0833    |\n",
      "|    value_loss           | 0.0898     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 99.9       |\n",
      "|    ep_rew_mean          | 1.91       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 635        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18313596 |\n",
      "|    clip_fraction        | 0.531      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.722     |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.0015     |\n",
      "|    loss                 | -0.0945    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0791    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17f7d1630>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.learn(total_timesteps=15000, callback=lvl2_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd0a5a",
   "metadata": {},
   "source": [
    "With just deploying the agent in a new envrionment, with the same action space (left, right and attack), we see a decent performance. This goes to show that the agent's policy is holding up in a new environment for the same rewards however the agent has a trigger finger which is wasteful in a dangerous environment. Deploying the agent in a more complex environment with a larger action space, while enabling the agent to prioritize actions to maximize reward will require reward shaping to incentivize (and penalize) actions resulting in a better player.\n",
    "\n",
    "## 7. Learning through interactions (Reward Shaping)\n",
    "\n",
    "Here we define 3 more game variables apart from ammo as seen in the first scenario. Using the documentation for VizDoom to retrieve the [list of game variables](https://github.com/Farama-Foundation/ViZDoom/blob/master/doc/Types.md#gamevariable), we add `hitcount`, `damage_taken` and `health` to nudge the agent's actions in the desired direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f2debc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_match = start_survival_game('deadly_corridor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cfbad1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.,   0.,   0.,  -1.])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = death_match.game.get_state()\n",
    "state.game_variables  # verify game variables passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "42353cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_match.end_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "da8539b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class start_survival_game(Env):\n",
    "    \n",
    "    # Initialization - creates a new environment, define observation and action space\n",
    "    def __init__(self, scenario, rendering = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        current_path = '/Users/y-unus/Documents/SS2023/CMSE890-730 - AML/Project - Doom RL'\n",
    "        cfg_file = os.path.join(current_path,scenario  + '.cfg')\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(cfg_file) # if you have not changed directories, mention the full path here\n",
    "\n",
    "        \n",
    "        if rendering == False:\n",
    "            self.game.set_window_visible(False) # Do not display the game window \n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "            \n",
    "        self.game.init() # start\n",
    "        \n",
    "            # get the frame resolution from the current game and push color channel to last position in the tuple\n",
    "#             self.obs_shape = self.game.get_state().screen_buffer.shape  # to read dynamic frame resolution; hardcoded for higer levels\n",
    "#             self.obs_shape =  self.obs_shape[1:] + (self.obs_shape[0],)\n",
    "        self.observation_space = Box(low = 0, high = 255, shape =  (100,160,1), dtype = np.uint8)\n",
    "        \n",
    "        self.action_space = Discrete(7) \n",
    "        \n",
    "        # initialize the new actions, zero damage and hits with full ammo at game start\n",
    "        \n",
    "        self.damage_taken = 0   \n",
    "        self.damagecount = 0\n",
    "        self.selected_weapon_ammo = 50\n",
    "\n",
    "\n",
    "    # Define action for exploration/ exploitation and return the next state, reward, done \n",
    "    # (game finished) and info (game variables available)\n",
    "    def step(self, action):\n",
    "\n",
    "        frame_skip = 4\n",
    "        \n",
    "    # Define the action space based on the 7 actions from the config file\n",
    "\n",
    "        actions = np.identity(7, dtype = np.uint8)\n",
    "        reward = self.game.make_action(actions[action], frame_skip)\n",
    "        cumulative_return = 0\n",
    "        \n",
    "        if self.game.get_state(): # game is running\n",
    "\n",
    "            state = self.game.get_state().screen_buffer  # acquire next frame\n",
    "            state = self.preprocessing(state) # grayscale\n",
    "            \n",
    "            game_variables = self.game.get_state().game_variables # unpack variables from the config file\n",
    "            health, damage_taken, damage_count, ammo = game_variables \n",
    "            \n",
    "            # Incentivize reward\n",
    "            acquired_damage = self.damage_taken - damage_taken # discourage\n",
    "            bullets_used = ammo - self.selected_weapon_ammo \n",
    "            accuracy = damage_count - self.damagecount # encourage damage to enemies\n",
    "            \n",
    "            # update the game variables\n",
    "            self.damage_taken = damage_taken\n",
    "            self.damagecount = damage_count\n",
    "            self.selected_weapon_ammo = ammo\n",
    "            \n",
    "            cumulative_return = reward + (10* bullets_used) + (25 * acquired_damage) + (250 * accuracy) \n",
    "            info = {\"Ammo\": ammo} \n",
    "            \n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape) # game has terminated\n",
    "            info = {\"Ammo\": 52} \n",
    "        \n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info\n",
    "    \n",
    "    # Reset parameters for new episode and return the state\n",
    "    def reset(self):\n",
    "        \n",
    "        self.damage_taken = 0   \n",
    "        self.damage_count = 0\n",
    "        self.selected_weapon_ammo = 50\n",
    "        \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        state = self.preprocessing(state) \n",
    "        return state\n",
    "    \n",
    "    # Convert the incoming frames to grayscale\n",
    "    def preprocessing(self, current_state):\n",
    "        observation = np.moveaxis(current_state, 0, -1)\n",
    "        greyscale_img = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        greyscale_img = cv2.resize( greyscale_img, (100,160), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(greyscale_img, (100,160,1))  # we remove the RGB color channel \n",
    "        return state\n",
    "\n",
    "    # closes game window after episodes are terminated to avoid manually closing it\n",
    "    def end_game(self):\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "011725da",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_checkpoint = './training/Death_Corridor'\n",
    "death_log = './logs/Death_Corridor'\n",
    "\n",
    "death_callback = save_my_model(check_freq=10000, save_path=death_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5370c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "death_train_model = PPO('CnnPolicy', death_match, tensorboard_log=death_log, verbose=1, learning_rate=0.0023, n_steps=1024, clip_range=0.15, gamma=0.91, \n",
    "                         gae_lambda=0.87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d6dc1bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/Death_Corridor/PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.6     |\n",
      "|    ep_rew_mean     | -82.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 32.4       |\n",
      "|    ep_rew_mean          | -70.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80457795 |\n",
      "|    clip_fraction        | 0.776      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.7       |\n",
      "|    explained_variance   | 0.000175   |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 367        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.121      |\n",
      "|    value_loss           | 658        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 34.4      |\n",
      "|    ep_rew_mean          | -68.5     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 21        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 145       |\n",
      "|    total_timesteps      | 3072      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9626027 |\n",
      "|    clip_fraction        | 0.797     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -1.45     |\n",
      "|    explained_variance   | 0.0756    |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 30        |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.145     |\n",
      "|    value_loss           | 262       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36.1       |\n",
      "|    ep_rew_mean          | -72.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 20         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20890275 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 35.7       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0696     |\n",
      "|    value_loss           | 257        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.3        |\n",
      "|    ep_rew_mean          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061971504 |\n",
      "|    clip_fraction        | 0.553       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 9.43        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0504      |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 38.7       |\n",
      "|    ep_rew_mean          | -89.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 309        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12325354 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.865     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 25.8       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | 0.0339     |\n",
      "|    value_loss           | 112        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 40.1       |\n",
      "|    ep_rew_mean          | -103       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49352413 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.947     |\n",
      "|    explained_variance   | 0.0425     |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0974     |\n",
      "|    value_loss           | 195        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36.2       |\n",
      "|    ep_rew_mean          | -105       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 420        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07115509 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.742     |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 24.5       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.0295     |\n",
      "|    value_loss           | 149        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 43.6       |\n",
      "|    ep_rew_mean          | -105       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 473        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07778117 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.794     |\n",
      "|    explained_variance   | 0.197      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 20.6       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.0191     |\n",
      "|    value_loss           | 107        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 40.5       |\n",
      "|    ep_rew_mean          | -113       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 529        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15053587 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.856     |\n",
      "|    explained_variance   | 0.444      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 6.16       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0372     |\n",
      "|    value_loss           | 57.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.8        |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 584         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059291687 |\n",
      "|    clip_fraction        | 0.468       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.0465      |\n",
      "|    value_loss           | 99.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 45.2       |\n",
      "|    ep_rew_mean          | -116       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 640        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23374204 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 7.36       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.0427     |\n",
      "|    value_loss           | 62.8       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 35.8       |\n",
      "|    ep_rew_mean          | -116       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 697        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02579617 |\n",
      "|    clip_fraction        | 0.0551     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.223     |\n",
      "|    explained_variance   | 0.21       |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 13.8       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.0104     |\n",
      "|    value_loss           | 142        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.9        |\n",
      "|    ep_rew_mean          | -116        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069982976 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.0872     |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 42.3        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00569     |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 25.2         |\n",
      "|    ep_rew_mean          | -116         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 810          |\n",
      "|    total_timesteps      | 15360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068666693 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.0348      |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 35.9         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.00349      |\n",
      "|    value_loss           | 149          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x179bf3d00>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_train_model.learn(total_timesteps = 15000, callback = death_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03602f44",
   "metadata": {},
   "source": [
    "As we can see from the above results from the training, the model is not able to learn adequately due to the harsh difficulty, from the trend of rewards it is clear that it is unlearning or unable to leverage experience despite reward shaping. This is because it is not able to survive in the harsh envrionment long enough.\n",
    "\n",
    "We can gradually train the agent over the same level by increasing the difficulty, the best agent from the previous level is trained on higher difficulty until it can perform adequately on this level. \n",
    "\n",
    "## 8. Curriculum Learning (Learning from the environment)\n",
    "\n",
    "For curriculum learning to work, we need to modify the config files by reducing the diffculty. Start from level 1 to 5 of the skill level and the agent's progression is carried over with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c2100880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "death_l1 = start_survival_game('deadly_corridor-L1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "de0462c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model_1 = PPO('CnnPolicy', death_l1, tensorboard_log=death_log, verbose=1, learning_rate=0.0023, n_steps=1024, clip_range=0.15, gamma=0.91,  gae_lambda=0.87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d247799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/Death_Corridor/PPO_8\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 269      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 138       |\n",
      "|    ep_rew_mean          | 119       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 22        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 90        |\n",
      "|    total_timesteps      | 2048      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3729077 |\n",
      "|    clip_fraction        | 0.51      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -1.87     |\n",
      "|    explained_variance   | -5.84e-05 |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 134       |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.0293    |\n",
      "|    value_loss           | 321       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 91.7      |\n",
      "|    ep_rew_mean          | 375       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 21        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 144       |\n",
      "|    total_timesteps      | 3072      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6975385 |\n",
      "|    clip_fraction        | 0.712     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -1.45     |\n",
      "|    explained_variance   | -0.0336   |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 225       |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.0638    |\n",
      "|    value_loss           | 591       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 72.6     |\n",
      "|    ep_rew_mean          | 919      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 20       |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 198      |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 7.67721  |\n",
      "|    clip_fraction        | 0.9      |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -0.283   |\n",
      "|    explained_variance   | 0.0285   |\n",
      "|    learning_rate        | 0.0023   |\n",
      "|    loss                 | 286      |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | 0.12     |\n",
      "|    value_loss           | 939      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 67.1       |\n",
      "|    ep_rew_mean          | 956        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 20         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 251        |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22416396 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0536    |\n",
      "|    explained_variance   | 0.0472     |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 4.87e+03   |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.00686    |\n",
      "|    value_loss           | 1.62e+04   |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 61.2     |\n",
      "|    ep_rew_mean          | 956      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 20       |\n",
      "|    iterations           | 6        |\n",
      "|    time_elapsed         | 305      |\n",
      "|    total_timesteps      | 6144     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.709075 |\n",
      "|    clip_fraction        | 0.344    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -0.147   |\n",
      "|    explained_variance   | 0.368    |\n",
      "|    learning_rate        | 0.0023   |\n",
      "|    loss                 | 1.18e+03 |\n",
      "|    n_updates            | 50       |\n",
      "|    policy_gradient_loss | 0.0106   |\n",
      "|    value_loss           | 7.9e+03  |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 45.8      |\n",
      "|    ep_rew_mean          | 1.28e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 19        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 358       |\n",
      "|    total_timesteps      | 7168      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4310156 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.0785   |\n",
      "|    explained_variance   | 0.481     |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 1.85e+03  |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.0392    |\n",
      "|    value_loss           | 6.92e+03  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 45         |\n",
      "|    ep_rew_mean          | 1.34e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 412        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14229156 |\n",
      "|    clip_fraction        | 0.0557     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0259    |\n",
      "|    explained_variance   | 0.701      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 736        |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.0213     |\n",
      "|    value_loss           | 4.85e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 44.6       |\n",
      "|    ep_rew_mean          | 1.41e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 467        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06714768 |\n",
      "|    clip_fraction        | 0.0213     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0164    |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 1.31e+03   |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.00279    |\n",
      "|    value_loss           | 6.99e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 44.4        |\n",
      "|    ep_rew_mean          | 1.64e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 522         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009689959 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.0193     |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 2.04e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00241     |\n",
      "|    value_loss           | 5.36e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 44.5         |\n",
      "|    ep_rew_mean          | 1.72e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 19           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 577          |\n",
      "|    total_timesteps      | 11264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055809226 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.024       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 508          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000245    |\n",
      "|    value_loss           | 4.69e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 44.1       |\n",
      "|    ep_rew_mean          | 1.85e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 631        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09115644 |\n",
      "|    clip_fraction        | 0.0407     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0207    |\n",
      "|    explained_variance   | 0.67       |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 1.25e+03   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.00319    |\n",
      "|    value_loss           | 7.52e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 43.8       |\n",
      "|    ep_rew_mean          | 1.95e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 687        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11270729 |\n",
      "|    clip_fraction        | 0.0162     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0055    |\n",
      "|    explained_variance   | 0.62       |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 1e+03      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.0183     |\n",
      "|    value_loss           | 5.36e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 42.5       |\n",
      "|    ep_rew_mean          | 2.02e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 743        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02035027 |\n",
      "|    clip_fraction        | 0.00527    |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.00316   |\n",
      "|    explained_variance   | 0.586      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 1.01e+03   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00299   |\n",
      "|    value_loss           | 6.68e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 42.2        |\n",
      "|    ep_rew_mean          | 2.09e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 799         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018449653 |\n",
      "|    clip_fraction        | 0.00654     |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.0027     |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 425         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    value_loss           | 5.11e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x179bf25c0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.learn(total_timesteps = 15000, callback = death_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2036810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17fb1d810>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preload the next model with weights (skipped L2)\n",
    "\n",
    "model_1.load('/Users/y-unus/Documents/SS2023/CMSE890-730 - AML/Project - Doom RL/training/Death_Corridor/training_model_30000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d858463f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/Death_Corridor/PPO_9\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.5     |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 34.8     |\n",
      "|    ep_rew_mean          | 927      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 22       |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 92       |\n",
      "|    total_timesteps      | 2048     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.164572 |\n",
      "|    clip_fraction        | 0.0347   |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -0.031   |\n",
      "|    explained_variance   | 0.196    |\n",
      "|    learning_rate        | 0.0023   |\n",
      "|    loss                 | 2.06e+03 |\n",
      "|    n_updates            | 160      |\n",
      "|    policy_gradient_loss | 0.0281   |\n",
      "|    value_loss           | 7.44e+03 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 34.7      |\n",
      "|    ep_rew_mean          | 968       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 20        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 147       |\n",
      "|    total_timesteps      | 3072      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6049529 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.147    |\n",
      "|    explained_variance   | 0.419     |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 995       |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | 0.0403    |\n",
      "|    value_loss           | 4.38e+03  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 35.4       |\n",
      "|    ep_rew_mean          | 1.03e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 20         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 203        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10312607 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0977    |\n",
      "|    explained_variance   | 0.525      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 581        |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    value_loss           | 5.21e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.3        |\n",
      "|    ep_rew_mean          | 1.04e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041255914 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.034      |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 575         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000264   |\n",
      "|    value_loss           | 4.98e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 34.4       |\n",
      "|    ep_rew_mean          | 1.09e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02555183 |\n",
      "|    clip_fraction        | 0.0325     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0275    |\n",
      "|    explained_variance   | 0.275      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 1.42e+03   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.00282    |\n",
      "|    value_loss           | 7.15e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 34.7       |\n",
      "|    ep_rew_mean          | 1.18e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 370        |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00068311 |\n",
      "|    clip_fraction        | 0.00391    |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.00359   |\n",
      "|    explained_variance   | -0.0335    |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 2.4e+03    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00147   |\n",
      "|    value_loss           | 8.48e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 35.3          |\n",
      "|    ep_rew_mean          | 1.22e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 19            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 426           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6966674e-06 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -0.000974     |\n",
      "|    explained_variance   | 0.374         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 527           |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000457     |\n",
      "|    value_loss           | 5.14e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.9        |\n",
      "|    ep_rew_mean          | 1.3e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 487         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008287551 |\n",
      "|    clip_fraction        | 0.00205     |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.00158    |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 815         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.000258    |\n",
      "|    value_loss           | 6e+03       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.3         |\n",
      "|    ep_rew_mean          | 1.34e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 547          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008243621 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.00234     |\n",
      "|    explained_variance   | 0.335        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 773          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 6.19e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.1        |\n",
      "|    ep_rew_mean          | 1.3e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010543412 |\n",
      "|    clip_fraction        | 0.00615     |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.00779    |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 528         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 6.01e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 35.5       |\n",
      "|    ep_rew_mean          | 1.22e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 664        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04302828 |\n",
      "|    clip_fraction        | 0.0205     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0168    |\n",
      "|    explained_variance   | 0.318      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 981        |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00485   |\n",
      "|    value_loss           | 6.06e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 35.2       |\n",
      "|    ep_rew_mean          | 1.14e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 720        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21985336 |\n",
      "|    clip_fraction        | 0.0753     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0674    |\n",
      "|    explained_variance   | 0.363      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 398        |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.00671   |\n",
      "|    value_loss           | 4.47e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 35.1       |\n",
      "|    ep_rew_mean          | 1.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 776        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27149704 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | 0.543      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 323        |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | 0.0165     |\n",
      "|    value_loss           | 3.2e+03    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 34.9       |\n",
      "|    ep_rew_mean          | 1.07e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 833        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15750238 |\n",
      "|    clip_fraction        | 0.0776     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.0761    |\n",
      "|    explained_variance   | 0.479      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 562        |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | 0.00245    |\n",
      "|    value_loss           | 4.18e+03   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x179bf25c0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_l3 = start_survival_game('deadly_corridor-L3')\n",
    "model_1.set_env(death_l3)\n",
    "model_1.learn(total_timesteps = 15000, callback = death_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eb3ae349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x29d808970>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preload the next model with weights\n",
    "\n",
    "model_1.load('/Users/y-unus/Documents/SS2023/CMSE890-730 - AML/Project - Doom RL/training/Death_Corridor/training_model_40000.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "64434b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/Death_Corridor/PPO_10\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.7     |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.1        |\n",
      "|    ep_rew_mean          | 1.11e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038272403 |\n",
      "|    clip_fraction        | 0.092       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 816         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 3.4e+03     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 34.8       |\n",
      "|    ep_rew_mean          | 1.13e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 137        |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13451514 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -0.161     |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 275        |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    value_loss           | 4.64e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.6        |\n",
      "|    ep_rew_mean          | 1.13e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079526454 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 519         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.000888   |\n",
      "|    value_loss           | 4.39e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.6        |\n",
      "|    ep_rew_mean          | 1.14e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 21          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024675425 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -0.0474     |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 442         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 4.6e+03     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 35.7      |\n",
      "|    ep_rew_mean          | 1.22e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 21        |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 291       |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0613244 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.0511   |\n",
      "|    explained_variance   | 0.646     |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 541       |\n",
      "|    n_updates            | 350       |\n",
      "|    policy_gradient_loss | 0.0392    |\n",
      "|    value_loss           | 3.11e+03  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 35.6          |\n",
      "|    ep_rew_mean          | 1.2e+03       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 20            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 346           |\n",
      "|    total_timesteps      | 7168          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0375166e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -0.000366     |\n",
      "|    explained_variance   | 0.408         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 852           |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000156     |\n",
      "|    value_loss           | 4.79e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.8         |\n",
      "|    ep_rew_mean          | 1.23e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 402          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.047776e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.28e-05    |\n",
      "|    explained_variance   | 0.378        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 493          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -5.44e-05    |\n",
      "|    value_loss           | 4.81e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.6         |\n",
      "|    ep_rew_mean          | 1.28e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 458          |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.460329e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -0.000512    |\n",
      "|    explained_variance   | 0.265        |\n",
      "|    learning_rate        | 0.0023       |\n",
      "|    loss                 | 517          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    value_loss           | 5.54e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 34.9          |\n",
      "|    ep_rew_mean          | 1.22e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 19            |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 514           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2109114e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -0.000328     |\n",
      "|    explained_variance   | 0.373         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 549           |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000164     |\n",
      "|    value_loss           | 4.78e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 34.9          |\n",
      "|    ep_rew_mean          | 1.17e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 19            |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 570           |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4174148e-06 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -0.000619     |\n",
      "|    explained_variance   | 0.467         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 350           |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.000232     |\n",
      "|    value_loss           | 3.32e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 34.7          |\n",
      "|    ep_rew_mean          | 1.14e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 19            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 626           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037988694 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -0.00103      |\n",
      "|    explained_variance   | 0.455         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 555           |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000834     |\n",
      "|    value_loss           | 3.4e+03       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 35        |\n",
      "|    ep_rew_mean          | 1.11e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 19        |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 682       |\n",
      "|    total_timesteps      | 13312     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2298822 |\n",
      "|    clip_fraction        | 0.043     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.0203   |\n",
      "|    explained_variance   | 0.436     |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 309       |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | -0.00399  |\n",
      "|    value_loss           | 3.69e+03  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 34.8      |\n",
      "|    ep_rew_mean          | 1.12e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 19        |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 737       |\n",
      "|    total_timesteps      | 14336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3631363 |\n",
      "|    clip_fraction        | 0.0898    |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -0.00406  |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.0023    |\n",
      "|    loss                 | 251       |\n",
      "|    n_updates            | 430       |\n",
      "|    policy_gradient_loss | 0.0445    |\n",
      "|    value_loss           | 2.52e+03  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 34.8          |\n",
      "|    ep_rew_mean          | 1.13e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 19            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 793           |\n",
      "|    total_timesteps      | 15360         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6670826e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.15          |\n",
      "|    entropy_loss         | -3.12e-05     |\n",
      "|    explained_variance   | 0.597         |\n",
      "|    learning_rate        | 0.0023        |\n",
      "|    loss                 | 226           |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -6.59e-06     |\n",
      "|    value_loss           | 2.92e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x179bf25c0>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_l4 = start_survival_game('deadly_corridor-L4')\n",
    "model_1.set_env(death_l4)\n",
    "model_1.learn(total_timesteps = 15000, callback = death_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
